{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfIm3MlDJ4PpD9F03MTAQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laura-turnbull-lloyd/STDH_teaching/blob/main/Lecture_4_analysing_spatial_patterns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Today we're going to be focussing on how to assess the spatial distribution of hazards, using landslides as an example.\n",
        "\n",
        "You'll be working with a spatial dataset consisting of polygons representing landslides that occurred during the 2008 Mw 7.9 Wenchuan earthquake in China.\n",
        "\n",
        "The data represent an area of 20 x 20 km.\n"
      ],
      "metadata": {
        "id": "qwtm067jNpVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Packages we'll be working with\n",
        "\n",
        "The main package we'll be working with is `GeoPandas`, which offers a pandas-like interface to working with geodata. Think of this as your tool for basic data manipulation and transformation, much like pandas.\n",
        "\n",
        "The other packages listed are those that you've worked with before, or packages that geopandas depends on.\n",
        "\n",
        "Let's load the packages:"
      ],
      "metadata": {
        "id": "kXT86n-5I7KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab environment (run once)\n",
        "!pip -q install \"geopandas>=0.14\" \"shapely>=2.0\" \"pyproj>=3.6\" \"rtree>=1.3.0\" \"seaborn>=0.13\" \"pointpats\"\n"
      ],
      "metadata": {
        "id": "YCzxsE2zo1FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, geopandas as gpd, matplotlib.pyplot as plt, seaborn as sns, requests, zipfile, io\n",
        "from shapely.geometry import Point, box, Polygon\n",
        "# to make sure that pandas outputs values in an easy to read format, we can customise the output format:\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
        "from pointpats import (\n",
        "    distance_statistics,\n",
        "    QStatistic,\n",
        "    random,\n",
        "    PointPattern\n",
        ")\n"
      ],
      "metadata": {
        "id": "-t_Ae73RpHEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Reading in landslide data\n",
        "The data have been provided for you in a shapefile format, in a zipped up file for you on GitHub. In the script below, you will automatically extract the data from GitHub (so no need to download it).\n",
        "\n",
        "In case you also wish to explore the data in a more traditional Geographical Infromation System (e.g. ArcPro or QGIS), I have also provided the data for you on Learn Ultra on this weeks page.\n",
        "\n",
        "\n",
        "To read in the data from GitHub:"
      ],
      "metadata": {
        "id": "R0Dcyk68EVG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read in data that's provided for you on Github\n",
        "url = \"https://github.com/laura-turnbull-lloyd/STDH_teaching/raw/main/landslides_clip.zip\"\n",
        "\n",
        "# Download the zip file into memory\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "# read in the shapefile\n",
        "z.extractall(\"landslides_clip\")\n",
        "LS = gpd.read_file(\"landslides_clip\") # this reads in the landslide data"
      ],
      "metadata": {
        "id": "AY97d6oC_NlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the first few lines of the data:"
      ],
      "metadata": {
        "id": "PFUCemGuWYmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# View the header info of the landslide data we've just read in\n",
        "print(LS.head())\n"
      ],
      "metadata": {
        "id": "orke1TRWGi-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively we can print out the whole dataset which will show us the first and last few lines in the dataset:"
      ],
      "metadata": {
        "id": "KJt2eUV0BeTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing out the whole dataset\n",
        "print(LS)"
      ],
      "metadata": {
        "id": "gWZEsp7nBlfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that there are 6604 rows in the data, which means there are 6604 individual landslide polygons.\n",
        "\n",
        "Note that python is a bit wierd in that it starts its index numbering from zero."
      ],
      "metadata": {
        "id": "INZEKSSa73T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each row in the landslide dataset contains a unique identifier (in column ID), the area of the polygon in $m^2$ (area_rob), the initials of the person who mapped it (data), and the geometry of the data (geometry). These attributes are linked to each individual landslide polygon.\n",
        "\n",
        "Using the `gpd.read_file`command, you read the data into a 'geodataframe', which is very similar to a traditional, non-spatial pandas DataFrame, but with an additional column called geometry."
      ],
      "metadata": {
        "id": "SUHh3tYPDLge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Determining the coordinate system of the data\n",
        "It's important to be aware of the coordinate system of the data you're working with. You can find it out using the ```crs``` command. It's easy to make mistakes in spatial data analysis when you aren't aware of the coordinate system of your data, and if it's an unsuitable coordinate system for the type of analysis you want to undertake.\n",
        "\n"
      ],
      "metadata": {
        "id": "wygZ_cxMXJem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS.crs"
      ],
      "metadata": {
        "id": "fE0L6lIYXWM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Plotting spatial data\n",
        "We can plot the raw spatial data using the code below. In this code, we undertake the following steps:\n",
        "1. Create a figure named `f` with one axis named `ax` by using the command plt.subplots (part of the library matplotlib, which we have imported at the top of the notebook).\n",
        "Note how the method is returning two elements and we can assign each of them to objects with different name (`f` and `ax`) by simply listing them at the front of the line, separated by commas.\n",
        "2. We plot the geographies and tell the function that we want it to draw the polygons on the axis we are passing, `ax`. This method returns the axis with the geographies in them, so we make sure to store it on an object with the same name, `ax`.\n",
        "3. We draw the entire plot by calling `plt.show()`.\n",
        "\n",
        "For more information on matplotlib plotting conventions, see [here](https://matplotlib.org/).\n",
        "\n",
        "You can also save figures outside of this notebook, using the ```plt.savefig``` command -- see below for an example. Any figures you save will be saved in the current working directory and therefore will need to be moved if you want to access them after the session has ended.\n",
        "\n"
      ],
      "metadata": {
        "id": "2wYXposAAWSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup figure and axis\n",
        "f, ax = plt.subplots(1, figsize=(16, 8))\n",
        "# Plot layer of polygons on the axis\n",
        "LS.plot(ax=ax)\n",
        "# Remove axis frames\n",
        "#ax.set_axis_off()\n",
        "# Display\n",
        "plt.show()\n",
        "\n",
        "# Save figure to a PNG file\n",
        "plt.savefig('Wenchuan_landslides.png')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# For a very high resolution image we can add the dpi in the command, e.g.\n",
        "#plt.savefig('Wenchuan_landslides.png', dpi = 1080) # I've left this line commented, as you don't need to run it for now.\n"
      ],
      "metadata": {
        "id": "kyiHN5vhLduz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Exploratory data analysis\n",
        "It's always useful to undertake exploratory data analysis to establish key characteristics about the data (hazard) that we're working with.\n",
        "\n",
        "We can use the pandas summary statistics (describe) function to be able to explore the data.\n",
        "\n",
        "The describe funciton gives the:\n",
        "> count\n",
        "\n",
        "> mean\n",
        "\n",
        "> standard deviation\n",
        "\n",
        "> minimum\n",
        "\n",
        "> 25 percentile\n",
        "\n",
        "> 50 percentile\n",
        "\n",
        "> 75 percentiles\n",
        "\n",
        "> max\n",
        "\n",
        "of all numeric columns in the dataset\n"
      ],
      "metadata": {
        "id": "OLjEMa39OjA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LS.describe()"
      ],
      "metadata": {
        "id": "N9N3ImhvD4My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's the maximum landslide size (including correct units)?**\n",
        "\n"
      ],
      "metadata": {
        "id": "eXsB-QXuPd1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For other summary infromation that is not given by the describe function, we can calculate this manually. For instance, we might be interested in the total area of landslides in the study area. We can calculate this using the pandas 'series sum' function."
      ],
      "metadata": {
        "id": "ZMPDkAMgI96s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_area = pd.Series(LS['area_rob'].sum(), index=['area_rob'])\n",
        "print(total_area)"
      ],
      "metadata": {
        "id": "2opH2fx0JLgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's the total area affected by landslides in the study area?**\n",
        "\n",
        "**What percentage of the entire study area is affected by landslides?**\n",
        "To answer this question you will have to undertake a further calculation which you can enter below:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HsGdT4SBIsL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your calculations here in order to answer the above question:"
      ],
      "metadata": {
        "id": "uiST0uhJXbPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how I did it, you can click 'show code' below, but do have a go at figuring this out for yourself first!"
      ],
      "metadata": {
        "id": "Amgv-0quXnZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "study_area = 20000*20000\n",
        "percent_LS = total_area/study_area*100\n",
        "percent_LS"
      ],
      "metadata": {
        "id": "qzrRQFugTIRU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Point pattern analysis: testing if there is a spatial pattern\n",
        "\n",
        "We can begin by asking a fundamental question: **Are landslides randomly distributed across the study area, or do they show spatial patterning?**\n",
        "\n",
        "To be able to use the toolbox of point pattern analysis answer this question, we first need to represent the data (in this case, landslides) as points.\n",
        "\n",
        "To do this we extract a point (the centroid) from the landslide polygons."
      ],
      "metadata": {
        "id": "Lt91m8Zm7_Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting points from the landslide polygons\n",
        "points = LS.copy()\n",
        "# change geometry\n",
        "points['geometry'] = points['geometry'].centroid\n",
        "points"
      ],
      "metadata": {
        "id": "waCUPau08KBo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've already established, the first step to get a sense of what the spatial dimension of this dataset looks like is to plot it."
      ],
      "metadata": {
        "id": "a1_zltQV8bFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1, ax = plt.subplots(1, figsize=(12, 8))\n",
        "points.plot(ax=ax) # this is the landslide data represented as points\n",
        "plt.autoscale(True)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "gDfHlU0F8aqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got the landslide data in a point format, we can test this observed point pattern against **complete spatial randomness (CSR)**, where there is neither clustering nor dispersion.\n",
        "\n",
        "We can do this by using the ```pointspats``` package to simulate CSR from a given point set, using the ```pointpats.random``` module, which creates a random dataset, based on the landslide dataset (i.e., it shares the same geomoetry and has the same number of points), that exhibits complete spatial randomness:\n"
      ],
      "metadata": {
        "id": "50A7NOSFXC58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create CSR from the point data set\n",
        "\n",
        "# first, we extract the x and y coordinates of each point\n",
        "points[\"x\"] = points.geometry.x\n",
        "points[\"y\"] = points.geometry.y\n",
        "\n",
        "# then we create a list of coordinates\n",
        "coordinates = points[[\"x\", \"y\"]].values\n",
        "\n",
        "# then we shuffle everything up\n",
        "random_pattern = random.poisson(coordinates, size=len(coordinates))"
      ],
      "metadata": {
        "id": "zbYxIsdrTz20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the random pattern with the\n",
        "observed landslide pattern."
      ],
      "metadata": {
        "id": "9Sfm6y8hr4dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f2, ax = plt.subplots(1, figsize=(9, 9))\n",
        "plt.scatter(\n",
        "    *coordinates.T,\n",
        "    color=\"k\",\n",
        "    marker=\".\",\n",
        "    label=\"Observed landslides\"\n",
        ")\n",
        "plt.scatter(*random_pattern.T, color=\"r\", marker=\"x\", label=\"Random\")\n",
        "ax.legend(ncol=1, loc=\"right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K0TNH76Vr-Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're going to test whether the landslide data exhibit a spatial pattern or not, by dividing the region into a grid of equal-size quadrats (cells) and then counting how many landslides fall in each cell.\n",
        "\n",
        "By examining whether observations are spread evenly over cells, this quadrat approach aims to estimate whether points are spread out, or if they are clustered into a few cells.\n",
        "\n",
        "In python, using the pointpats package, we can visualize the point data, and how the points are counted within each grid cell. We can do this using the  ``QStatistic.plot()`` method."
      ],
      "metadata": {
        "id": "CW4IJLZasIYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qstat = QStatistic(coordinates, nx=15, ny=15) # nx and ny are the number of cells in the x and y directions\n",
        "qstat.plot()"
      ],
      "metadata": {
        "id": "aDP1sCMWsjW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, for a 15 x 15 grid (note that this is specified in the code block above) spanning the point pattern, we can see a mix of very high values and very low values.\n",
        "\n",
        "We can use the $X^2$ test statistic to test if the pattern is random or not, which is expressed as:\n",
        "$$\n",
        "\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n",
        "$$\n",
        "where $O_i$ is the observed count in quadrat 𝑖, $E_i$ is the expected count under CSR (same overall intensity), and $k$ is the number of quadrats.\n",
        "\n",
        "If  $X^2$ is small and the p-value is large (p > 0.05), the pattern is consistent with CSR.\n",
        "\n",
        "If  $X^2$ is large and p < 0.05, the pattern departs from randomness, suggesting clustering or regular spacing.\n",
        "\n",
        "At this point, the test tells us whether the pattern is clustered, but not where or by how much.\n",
        "\n",
        "Let's have a look at the $X^2$ statistics for the landslide point dataset...\n"
      ],
      "metadata": {
        "id": "iGbxEexMswyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"chi2 p value\", qstat.chi2_pvalue)\n",
        "print(\"chi2 value\", qstat.chi2)"
      ],
      "metadata": {
        "id": "NwubURgHs-ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a significant P-value and a very high $X^2$ statistic, telling us that the landslide data significantly deviate from CSR.  \n",
        "\n",
        "Now, let's repeat the analysis for the randomly distributed dataset that we generated, and view the resulting stats:"
      ],
      "metadata": {
        "id": "8DZ_uOUms7RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qstat_rand = QStatistic(random_pattern, nx=15, ny=15)\n",
        "qstat_rand.plot()\n",
        "print(\"chi2 p value\", qstat_rand.chi2_pvalue)\n",
        "print(\"chi2 value\", qstat_rand.chi2)"
      ],
      "metadata": {
        "id": "g8dS3NafKKJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does this analysis show? Can you think of any potential issues with the above analysis?**"
      ],
      "metadata": {
        "id": "727wy-J9KdKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Take home message:** At this point, this analysis tells is us whether the pattern is clustered (across the whole study area), but not where or by how much."
      ],
      "metadata": {
        "id": "SSu8ZRqrQXnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Visualizing patterns more clearly: point density\n",
        "\n",
        "We can extend the quadrat approach used in the previous section, where we counted the number of landslides in each cell, to view it as a spatial or 2-D histogram.\n",
        "\n",
        "To do this from scratch, we generate a regular grid (either squared or hexagonal), and count how many points fall within each grid cell. This is attractive because it is simple and intuitive.\n",
        "\n",
        "First, let's create the regular grid."
      ],
      "metadata": {
        "id": "D9V-qiwKQ1So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total area for the grid\n",
        "xmin, ymin, xmax, ymax= LS.total_bounds # this function extracts the bounding coordinates of the landslide data\n",
        "width = 500 # this sets the size of the grid cell width\n",
        "height = 500 # this sets the size of the grid cell width\n",
        "# projection of the grid - this sets the crs to be the same as the data we area already working with\n",
        "crs = LS.crs\n",
        "rows = int(np.ceil((ymax-ymin) /  height))\n",
        "cols = int(np.ceil((xmax-xmin) / width))\n",
        "XleftOrigin = xmin\n",
        "XrightOrigin = xmin + width\n",
        "YtopOrigin = ymax\n",
        "YbottomOrigin = ymax- height\n",
        "polygons = []\n",
        "for i in range(cols):\n",
        "   Ytop = YtopOrigin\n",
        "   Ybottom =YbottomOrigin\n",
        "   for j in range(rows):\n",
        "       polygons.append(Polygon([(XleftOrigin, Ytop), (XrightOrigin, Ytop), (XrightOrigin, Ybottom), (XleftOrigin, Ybottom)]))\n",
        "       Ytop = Ytop - height\n",
        "       Ybottom = Ybottom - height\n",
        "   XleftOrigin = XleftOrigin + width\n",
        "   XrightOrigin = XrightOrigin + width\n",
        "\n",
        "grid = gpd.GeoDataFrame({'geometry':polygons}, crs=crs)\n",
        "grid"
      ],
      "metadata": {
        "id": "sQTCgLlkWRcM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a new object called `landslide_count` and store the result of a spatial join between landslide points and the grid."
      ],
      "metadata": {
        "id": "XnQwgP8F8PMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count = gpd.sjoin(grid, points, how='left', predicate ='contains')\n",
        "landslide_count"
      ],
      "metadata": {
        "id": "2dvLvfiE8om9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above table, NaN means \"Not a number\", and this happens when the count function didn't count a landslide in a particular cell."
      ],
      "metadata": {
        "id": "dId-LFhMFz1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each landslide carries the ID of its corresponding grid cell (on the far left of the above table). Let's explicitly add this grid cell ID to the geodataframe:"
      ],
      "metadata": {
        "id": "KlkdxOdK9VRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count['grid_index'] = landslide_count.index\n",
        "landslide_count"
      ],
      "metadata": {
        "id": "SCs4d7kL9lgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to count the number of landslides in each grid cell. We can do this using the dissolve tool (which simplifies data), counting the number of landslides within each grid cell."
      ],
      "metadata": {
        "id": "SQBIit1qPTVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "landslide_count_dissolve = landslide_count.dissolve(by='grid_index', aggfunc='count')\n",
        "landslide_count_dissolve"
      ],
      "metadata": {
        "id": "_e2dNicLPjTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the table above, you can see that for the attributes that were associated with the landslide point data, you now have the 'count' of how many landslides there were in each grid cell.\n",
        "\n",
        "So, for example, for grid cell with an index of 3, there were 7 landslide points within that cell. Whilst for grid cell with an index of 0, there were 0 landslides.\n",
        "\n",
        "We can now plot out the resulting landslide point density map."
      ],
      "metadata": {
        "id": "d3Mc7GN8QjZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, figsize=(12, 8))\n",
        "landslide_count_dissolve.plot(ax=ax, column='index_right', cmap='jet', legend = True) # here, cmap = 'jet' specifies the colour ramp I've chosen to use.\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid"
      ],
      "metadata": {
        "id": "gvxB3heoRB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Visualizing patterns more clearly: kernel density estimate\n",
        "\n",
        "We can also summarise the spatial distribution of points using a kernel density function. This is simple to achieve using the `seaborn package`.\n",
        "\n",
        "A kernel density function replaces plotting every single point by estimating the continuous observed probability distribution. It's not too dissimilar to the point density counts that you've just carried out, but it differs in that it creates a surface that models the probability of point density over space.\n",
        "\n",
        "The idea behind kernel density estimates (KDEs) is to count the number of points in a continuous way. Instead of using discrete counting, where you include a point in the count if it is inside a certain boundary and ignore it otherwise, KDEs use functions (kernels) that include points but give different weights to each one depending on how far from the location where we are counting the point is.\n",
        "\n",
        "Creating a kernel density estimate is very straightfoward in Python. In its simplest form, we can run the following single line of code:"
      ],
      "metadata": {
        "id": "Pj5N5hUCW_6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data = points, x = \"x\", y = \"y\", fill=True, cmap='viridis', cbar = True)"
      ],
      "metadata": {
        "id": "vdfsvYbQXqEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much like with the bin size in the histogram, the ability of the kernel density estimate to accurately represent the data depends on the choice of the bandwidth, which essentially represents the radius of smoothing.\n",
        "\n",
        "An over-smoothed estimate might erase meaningful patterns, but an under-smoothed estimate can obscure the true patterns within random noise.\n",
        "\n",
        "The easiest way to check the robustness of the estimate is to adjust the default bandwidth (which is a backend calculation in seaborn using scipy, and is based on the distribution of the data).\n",
        "\n",
        "To do this, we can add a bw_adjust function (```bw_adjust = 0.5```) to the code below, and have a play around to see how different values of bw_adjust alter the resulting kernel density estimation."
      ],
      "metadata": {
        "id": "B5ikPP-qdHYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data = points, x = \"x\", y = \"y\", fill=True, cmap='viridis', cbar = True, bw_adjust = 0.5)"
      ],
      "metadata": {
        "id": "SPvavviRbvR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can read more about the different parameter options for kernel density estimates using the seaborn package [here](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)."
      ],
      "metadata": {
        "id": "-wfwz4JKafcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary:\n",
        "\n",
        "A KDE with a small bandwidth → fine detail, local patterns\n",
        "*   Produces a highly detailed surface with many small peaks\n",
        "*   Highlights local clusters and sharp changes in density\n",
        "*   May appear noisy — individual landslides dominate the surface\n",
        "*   Useful for identifying hotspots at smaller scales\n",
        "\n",
        "A KDE with a larger bandwidth → broad smoothing, regional trends\n",
        "*   Each landslide influences a larger area → generalized pattern\n",
        "*   Emphasizes broader-scale patterns\n",
        "*   Can mask local variability and small clusters"
      ],
      "metadata": {
        "id": "TqzfC34QfjYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Area density measures\n",
        "\n",
        "Point density measures treats every landslide equally, regardless of their size.\n",
        "Often, we're keen to understand the overall magnitude of a hazard, not just whether or not an event has occurred. For hazards such as landslides and wildfires, the area (i.e. the footprint of the event) is a good place to start.\n",
        "\n",
        "Similar to point density calculations, to undertake area density calculations, we need to use a grid to divide up the study area into cells. You already created a grid to use in the point density calculations, so we'll just use that same grid."
      ],
      "metadata": {
        "id": "gQwMB7EmV1Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the grid over the data shows how the landslides are generally clustered into a few regions and we’ll end up with many “empty” grid cells:"
      ],
      "metadata": {
        "id": "eHzi-HEBY4oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f3, ax = plt.subplots(1, figsize=(12, 8))\n",
        "LS.plot(ax=ax) # this is the landslide data\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid!\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "nqzJIqV4Y-vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’re going to determine the area of landslides in each square polygon. This takes a couple of steps in `GeoPandas`.\n",
        "\n",
        "First, we use the intersect tool to divide up the landslide polygons based on the grid cells.\n",
        "\n",
        "Then we calculate the area of each fragmented landslide polygon, and add the landslide index to the dataframe (just to make sure it gets carried over into the subsequent analysis)."
      ],
      "metadata": {
        "id": "cXVONUlmJMNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LS_intersection = LS.overlay(cell, keep_geom_type = False)\n",
        "LS_intersect = LS.overlay(grid, how='intersection', keep_geom_type=False)\n",
        "LS_intersect['LS_area'] =LS_intersect.apply(lambda row: row.geometry.area,axis=1) # calculate the area of each landslide segment\n",
        "LS_intersect['LS_index'] = LS_intersect.index\n",
        "print(LS_intersect)\n"
      ],
      "metadata": {
        "id": "zLToCM9Zij75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot out the data to check it looks sensible..."
      ],
      "metadata": {
        "id": "SgIUXbNWxL_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f4, ax = plt.subplots(1, figsize=(25, 15))\n",
        "LS_intersect.plot(ax=ax, facecolor = \"none\", edgecolor = 'red')\n",
        "plt.autoscale(True)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "zYNhf4qYjFus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks good. You can clearly see where landslide polygons have been divided up where they cross grid cells."
      ],
      "metadata": {
        "id": "9ZjhIH17xXdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next task is to undertake a spatial join, to join the landslide data to the grid. We do this using the geopandas spatial join, ```sjoin``` tool.\n",
        "\n"
      ],
      "metadata": {
        "id": "xASaDDhTxev_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged = gpd.sjoin(grid, LS_intersect, predicate = 'contains', how='left')\n",
        "merged"
      ],
      "metadata": {
        "id": "qyIShIwWaaOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did before, we'll add the grid index to the dataframe:"
      ],
      "metadata": {
        "id": "_vozWSH5xtYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged['grid_index'] = merged.index # here, we add the index of the cells covering the study area into the dataframe so that we can use this in our analysis"
      ],
      "metadata": {
        "id": "7zJYxM5hntVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged)"
      ],
      "metadata": {
        "id": "6SNhYEGCc6W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the dataframe you've just printed out, you can see that some landslides have the same grid index -- this is because there is more than one landslide in a signle grid cell. What we want is the total area of landslides in a single grid cell. To achieve this, we can dissolve the data, in which we combine information (landslide area) from individual landslide polygons, to generate a total landslide area per grid cell."
      ],
      "metadata": {
        "id": "5-StOQvwkU4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute stats per grid cell -- aggregate landslides to grid cells with dissolve\n",
        "dissolve_merged = merged.dissolve(by='grid_index', aggfunc='sum') # sums up all the values in the area column\n",
        "dissolve_merged"
      ],
      "metadata": {
        "id": "-45r5Ycjkk7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, to help with landslide area density calculations, we need the total area of each grid cell. There are multiple ways we can calculate this (we specified the dimensions of each grid cell earlier). I always like to calculate dimensions based on the actual data, as there's less room for error."
      ],
      "metadata": {
        "id": "oTDxZ9bSy5CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dissolve_merged['cellarea'] =dissolve_merged.apply(lambda row: row.geometry.area,axis=1)# determine the area of each cell\n",
        "dissolve_merged"
      ],
      "metadata": {
        "id": "pA4TuPCwuDs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you've got the area of landsides in each grid cell you can finally calculate the landslide area density, which is simply the area of landslides in each cell, divided by the area of that cell, multiplied by 100, to give landslide area as a %.\n"
      ],
      "metadata": {
        "id": "12zBDNxb0HkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dissolve_merged['area_density'] = (dissolve_merged['LS_area']/dissolve_merged['cellarea'])*100\n",
        "dissolve_merged"
      ],
      "metadata": {
        "id": "rpSnB1bx1lk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to plot out the end product of this analysis:"
      ],
      "metadata": {
        "id": "Fvf81kSwzzAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f5, ax = plt.subplots(1, figsize=(12, 8))\n",
        "dissolve_merged.plot(ax=ax, column='area_density', cmap='jet', legend = True) # here, cmap = 'jet' specifies the colour ramp I've chosen to use.\n",
        "plt.autoscale(True)\n",
        "grid.plot(ax=ax, facecolor=\"none\", edgecolor='grey') # this is where you overlay the grid\n"
      ],
      "metadata": {
        "id": "nsb0LoWik8I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Compare the different approaches to assess spatial patterns of landslides\n",
        "\n",
        "You've used point density, kernel density and area density measures to assess the spatial patterns of landslides.\n",
        "\n",
        "Compare and contrast the resulting patterns of landslides from these different types of analyses.\n",
        "\n",
        "Have a think about which approach is most useful and why, in consideration of different hazards you're interested in, and in consideration of different data formats available to us (e.g. point/polygon/line representations of hazards).\n",
        "\n"
      ],
      "metadata": {
        "id": "qtwHDAn2Ro05"
      }
    }
  ]
}